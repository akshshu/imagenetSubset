{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9YLkUn8iKqtEmMoXSjJu5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshshu/imagenetSubset/blob/master/imsubset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWHqG1fpqD_m",
        "outputId": "0ef00359-6027-4ffb-9e75-ff33cff0d166"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7ZXXqZJs2G-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190a12de-f0a2-478f-e484-942289f3044d"
      },
      "source": [
        "# #!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import requests\n",
        "import argparse\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import csv\n",
        "\n",
        "from multiprocessing import Pool, Process, Value, Lock\n",
        "\n",
        "from requests.exceptions import ConnectionError, ReadTimeout, TooManyRedirects, MissingSchema, InvalidURL\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='ImageNet image scraper')\n",
        "# parser.add_argument('-scrape_only_flickr', default=True, type=lambda x: (str(x).lower() == 'true'))\n",
        "scrape_only_flickr = True\n",
        "# parser.add_argument('-number_of_classes', default = 10, type=int)\n",
        "number_of_classes = 50\n",
        "# parser.add_argument('-images_per_class', default = 10, type=int)\n",
        "images_per_class = 3\n",
        "# parser.add_argument('-data_root', default='' , type=str)\n",
        "data_root = \"/content/gdrive/MyDrive\"\n",
        "# parser.add_argument('-use_class_list', default=False,type=lambda x: (str(x).lower() == 'true'))\n",
        "use_class_list = False\n",
        "class_list = [\"n00017222\", \"n04465501\"]\n",
        "# parser.add_argument('-class_list', default=[], nargs='*')\n",
        "# parser.add_argument('-debug', default=False,type=lambda x: (str(x).lower() == 'true'))\n",
        "debug = False\n",
        "multiprocessing_workers = 1\n",
        "# parser.add_argument('-multiprocessing_workers', default = 8, type=int)\n",
        "\n",
        "# args, args_other = parser.parse_known_args()\n",
        "\n",
        "if debug:\n",
        "    logging.basicConfig(filename='imagenet_scarper.log', level=logging.DEBUG)\n",
        "\n",
        "if len(data_root) == 0:\n",
        "    logging.error(\"-data_root is required to run downloader!\")\n",
        "    exit()\n",
        "\n",
        "if not os.path.isdir(data_root):\n",
        "    logging.error(\n",
        "        f'folder {data_root} does not exist! please provide existing folder in -data_root arg!')\n",
        "    exit()\n",
        "\n",
        "\n",
        "def IMAGENET_API_WNID_TO_URLS(\n",
        "    wnid): return f'http://www.image-net.org/api/imagenet.synset.geturls?wnid={wnid}'\n",
        "\n",
        "\n",
        "# current_folder = os.path.dirname(os.path.realpath(__file__))\n",
        "current_folder = os.getcwd()\n",
        "\n",
        "class_info_json_filename = 'imagenet_class_info.json'\n",
        "class_info_json_filepath = os.path.join(\n",
        "    current_folder, class_info_json_filename)\n",
        "\n",
        "class_info_dict = dict()\n",
        "\n",
        "with open(class_info_json_filepath) as class_info_json_f:\n",
        "    class_info_dict = json.load(class_info_json_f)\n",
        "\n",
        "classes_to_scrape = []\n",
        "\n",
        "if use_class_list == True:\n",
        "    for item in class_list:\n",
        "        classes_to_scrape.append(item)\n",
        "        if item not in class_info_dict:\n",
        "            logging.error(f'Class {item} not found in ImageNete')\n",
        "            exit()\n",
        "\n",
        "elif use_class_list == False:\n",
        "    potential_class_pool = []\n",
        "    for key, val in class_info_dict.items():\n",
        "\n",
        "        if scrape_only_flickr:\n",
        "            if int(val['flickr_img_url_count']) * 0.9 > images_per_class:\n",
        "                potential_class_pool.append(key)\n",
        "        else:\n",
        "            if int(val['img_url_count']) * 0.8 > images_per_class:\n",
        "                potential_class_pool.append(key)\n",
        "\n",
        "    if (len(potential_class_pool) < number_of_classes):\n",
        "        logging.error(\n",
        "            f\"With {images_per_class} images per class there are {len(potential_class_pool)} to choose from.\")\n",
        "        logging.error(\n",
        "            f\"Decrease number of classes or decrease images per class.\")\n",
        "        exit()\n",
        "\n",
        "    picked_classes_idxes = np.random.choice(\n",
        "        len(potential_class_pool), number_of_classes, replace=False)\n",
        "\n",
        "    for idx in picked_classes_idxes:\n",
        "        classes_to_scrape.append(potential_class_pool[idx])\n",
        "\n",
        "\n",
        "print(\"Picked the following clases:\")\n",
        "print([class_info_dict[class_wnid]['class_name']\n",
        "      for class_wnid in classes_to_scrape])\n",
        "\n",
        "imagenet_images_folder = os.path.join(data_root, 'imagenet_images')\n",
        "if not os.path.isdir(imagenet_images_folder):\n",
        "    os.mkdir(imagenet_images_folder)\n",
        "\n",
        "\n",
        "scraping_stats = dict(\n",
        "    all=dict(\n",
        "        tried=0,\n",
        "        success=0,\n",
        "        time_spent=0,\n",
        "    ),\n",
        "    is_flickr=dict(\n",
        "        tried=0,\n",
        "        success=0,\n",
        "        time_spent=0,\n",
        "    ),\n",
        "    not_flickr=dict(\n",
        "        tried=0,\n",
        "        success=0,\n",
        "        time_spent=0,\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "def add_debug_csv_row(row):\n",
        "    with open('stats.csv', \"a\") as csv_f:\n",
        "        csv_writer = csv.writer(csv_f, delimiter=\",\")\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "\n",
        "class MultiStats():\n",
        "    def __init__(self):\n",
        "\n",
        "        self.lock = Lock()\n",
        "\n",
        "        self.stats = dict(\n",
        "            all=dict(\n",
        "                tried=Value('d', 0),\n",
        "                success=Value('d', 0),\n",
        "                time_spent=Value('d', 0),\n",
        "            ),\n",
        "            is_flickr=dict(\n",
        "                tried=Value('d', 0),\n",
        "                success=Value('d', 0),\n",
        "                time_spent=Value('d', 0),\n",
        "            ),\n",
        "            not_flickr=dict(\n",
        "                tried=Value('d', 0),\n",
        "                success=Value('d', 0),\n",
        "                time_spent=Value('d', 0),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def inc(self, cls, stat, val):\n",
        "        with self.lock:\n",
        "            self.stats[cls][stat].value += val\n",
        "\n",
        "    def get(self, cls, stat):\n",
        "        with self.lock:\n",
        "            ret = self.stats[cls][stat].value\n",
        "        return ret\n",
        "\n",
        "\n",
        "multi_stats = MultiStats()\n",
        "\n",
        "\n",
        "if debug:\n",
        "    row = [\n",
        "        \"all_tried\",\n",
        "        \"all_success\",\n",
        "        \"all_time_spent\",\n",
        "        \"is_flickr_tried\",\n",
        "        \"is_flickr_success\",\n",
        "        \"is_flickr_time_spent\",\n",
        "        \"not_flickr_tried\",\n",
        "        \"not_flickr_success\",\n",
        "        \"not_flickr_time_spent\"\n",
        "    ]\n",
        "    add_debug_csv_row(row)\n",
        "\n",
        "\n",
        "def add_stats_to_debug_csv():\n",
        "    row = [\n",
        "        multi_stats.get('all', 'tried'),\n",
        "        multi_stats.get('all', 'success'),\n",
        "        multi_stats.get('all', 'time_spent'),\n",
        "        multi_stats.get('is_flickr', 'tried'),\n",
        "        multi_stats.get('is_flickr', 'success'),\n",
        "        multi_stats.get('is_flickr', 'time_spent'),\n",
        "        multi_stats.get('not_flickr', 'tried'),\n",
        "        multi_stats.get('not_flickr', 'success'),\n",
        "        multi_stats.get('not_flickr', 'time_spent'),\n",
        "    ]\n",
        "    add_debug_csv_row(row)\n",
        "\n",
        "\n",
        "def print_stats(cls, print_func):\n",
        "\n",
        "    actual_all_time_spent = time.time() - scraping_t_start.value\n",
        "    processes_all_time_spent = multi_stats.get('all', 'time_spent')\n",
        "\n",
        "    if processes_all_time_spent == 0:\n",
        "        actual_processes_ratio = 1.0\n",
        "    else:\n",
        "        actual_processes_ratio = actual_all_time_spent / processes_all_time_spent\n",
        "\n",
        "    #print(f\"actual all time: {actual_all_time_spent} proc all time {processes_all_time_spent}\")\n",
        "\n",
        "    print_func(f'STATS For class {cls}:')\n",
        "    print_func(f' tried {multi_stats.get(cls, \"tried\")} urls with'\n",
        "               f' {multi_stats.get(cls, \"success\")} successes')\n",
        "\n",
        "    if multi_stats.get(cls, \"tried\") > 0:\n",
        "        print_func(\n",
        "            f'{100.0 * multi_stats.get(cls, \"success\")/multi_stats.get(cls, \"tried\")}% success rate for {cls} urls ')\n",
        "    if multi_stats.get(cls, \"success\") > 0:\n",
        "        print_func(f'{multi_stats.get(cls,\"time_spent\") * actual_processes_ratio / multi_stats.get(cls,\"success\")} seconds spent per {cls} succesful image download')\n",
        "\n",
        "\n",
        "lock = Lock()\n",
        "url_tries = Value('d', 0)\n",
        "scraping_t_start = Value('d', time.time())\n",
        "class_folder = ''\n",
        "class_images = Value('d', 0)\n",
        "\n",
        "\n",
        "def get_image(img_url):\n",
        "\n",
        "    # print(f'Processing {img_url}')\n",
        "\n",
        "    # time.sleep(3)\n",
        "\n",
        "    if len(img_url) <= 1:\n",
        "        return\n",
        "\n",
        "    cls_imgs = 0\n",
        "    with lock:\n",
        "        cls_imgs = class_images.value\n",
        "\n",
        "    if cls_imgs >= images_per_class:\n",
        "        return\n",
        "\n",
        "    logging.debug(img_url)\n",
        "\n",
        "    cls = ''\n",
        "\n",
        "    if 'flickr' in img_url:\n",
        "        cls = 'is_flickr'\n",
        "    else:\n",
        "        cls = 'not_flickr'\n",
        "        if scrape_only_flickr:\n",
        "            return\n",
        "\n",
        "    t_start = time.time()\n",
        "\n",
        "    def finish(status):\n",
        "        t_spent = time.time() - t_start\n",
        "        multi_stats.inc(cls, 'time_spent', t_spent)\n",
        "        multi_stats.inc('all', 'time_spent', t_spent)\n",
        "\n",
        "        multi_stats.inc(cls, 'tried', 1)\n",
        "        multi_stats.inc('all', 'tried', 1)\n",
        "\n",
        "        if status == 'success':\n",
        "            multi_stats.inc(cls, 'success', 1)\n",
        "            multi_stats.inc('all', 'success', 1)\n",
        "\n",
        "        elif status == 'failure':\n",
        "            pass\n",
        "        else:\n",
        "            logging.error(f'No such status {status}!!')\n",
        "            exit()\n",
        "        return\n",
        "\n",
        "    with lock:\n",
        "        url_tries.value += 1\n",
        "        if url_tries.value % 250 == 0:\n",
        "            print(f'\\nScraping stats:')\n",
        "            print_stats('is_flickr', print)\n",
        "            print_stats('not_flickr', print)\n",
        "            print_stats('all', print)\n",
        "            if debug:\n",
        "                add_stats_to_debug_csv()\n",
        "\n",
        "    try:\n",
        "        img_resp = requests.get(img_url, timeout=1)\n",
        "    except ConnectionError:\n",
        "        logging.debug(f\"Connection Error for url {img_url}\")\n",
        "        return finish('failure')\n",
        "    except ReadTimeout:\n",
        "        logging.debug(f\"Read Timeout for url {img_url}\")\n",
        "        return finish('failure')\n",
        "    except TooManyRedirects:\n",
        "        logging.debug(f\"Too many redirects {img_url}\")\n",
        "        return finish('failure')\n",
        "    except MissingSchema:\n",
        "        return finish('failure')\n",
        "    except InvalidURL:\n",
        "        return finish('failure')\n",
        "\n",
        "    if not 'content-type' in img_resp.headers:\n",
        "        return finish('failure')\n",
        "\n",
        "    if not 'image' in img_resp.headers['content-type']:\n",
        "        logging.debug(\"Not an image\")\n",
        "        return finish('failure')\n",
        "\n",
        "    if (len(img_resp.content) < 1000):\n",
        "        return finish('failure')\n",
        "\n",
        "    logging.debug(img_resp.headers['content-type'])\n",
        "    logging.debug(f'image size {len(img_resp.content)}')\n",
        "\n",
        "    img_name = img_url.split('/')[-1]\n",
        "    img_name = img_name.split(\"?\")[0]\n",
        "\n",
        "    if (len(img_name) <= 1):\n",
        "        return finish('failure')\n",
        "\n",
        "    img_file_path = os.path.join(imagenet_images_folder, img_name)\n",
        "    logging.debug(f'Saving image in {img_file_path}')\n",
        "\n",
        "    with open(img_file_path, 'wb') as img_f:\n",
        "        img_f.write(img_resp.content)\n",
        "\n",
        "        with lock:\n",
        "            class_images.value += 1\n",
        "\n",
        "        logging.debug(f'Scraping stats')\n",
        "        print_stats('is_flickr', logging.debug)\n",
        "        print_stats('not_flickr', logging.debug)\n",
        "        print_stats('all', logging.debug)\n",
        "\n",
        "        return finish('success')\n",
        "\n",
        "\n",
        "for class_wnid in classes_to_scrape:\n",
        "\n",
        "    class_name = class_info_dict[class_wnid][\"class_name\"]\n",
        "    print(f'Scraping images for class \\\"{class_name}\\\"')\n",
        "    url_urls = IMAGENET_API_WNID_TO_URLS(class_wnid)\n",
        "\n",
        "    # time.sleep(0.2)\n",
        "    resp = requests.get(url_urls)\n",
        "\n",
        "    class_folder = os.path.join(imagenet_images_folder, class_name)\n",
        "    # if not os.path.exists(class_folder):\n",
        "    #     os.mkdir(class_folder)\n",
        "\n",
        "    class_images.value = 0\n",
        "\n",
        "    urls = [url.decode('utf-8') for url in resp.content.splitlines()]\n",
        "\n",
        "    for url in urls:\n",
        "        get_image(url)\n",
        "\n",
        "    # print(f\"Multiprocessing workers: {multiprocessing_workers}\")\n",
        "    # with Pool(processes=multiprocessing_workers) as p:\n",
        "    #     p.map(get_image, urls)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Picked the following clases:\n",
            "['bus stop', 'green peafowl', 'strafer', 'field sparrow', 'cranberry bush', 'Sussex spaniel', 'hasty pudding', 'back porch', 'drip loop', 'firstborn', 'stiff', 'dasyurid marsupial', 'sexton', 'horse trader', 'pitching wedge', 'Elamite', 'horse wrangler', 'Brabancon griffon', 'farm boy', 'whooper', 'annual fern', 'jewels-of-opar', 'windshield', 'mosquitofish', 'two-eyed violet', 'blanket', 'etui', 'pinstripe', 'feather boa', 'girder', 'Confederate', 'silverspot', 'punch bowl', 'sowbread', 'chamberpot', 'high-protein diet', 'Orpington', 'western tanager', 'duffel', 'flat-coated retriever', 'emperor butterfly', 'Chilean firebush', 'automobile engine', 'screw', 'gill fungus', 'bladderpod', 'willowherb', 'wandering albatross', 'white sauce', 'potter wasp']\n",
            "Scraping images for class \"bus stop\"\n",
            "Scraping images for class \"green peafowl\"\n",
            "Scraping images for class \"strafer\"\n",
            "Scraping images for class \"field sparrow\"\n",
            "Scraping images for class \"cranberry bush\"\n",
            "Scraping images for class \"Sussex spaniel\"\n",
            "Scraping images for class \"hasty pudding\"\n",
            "Scraping images for class \"back porch\"\n",
            "Scraping images for class \"drip loop\"\n",
            "Scraping images for class \"firstborn\"\n",
            "Scraping images for class \"stiff\"\n",
            "Scraping images for class \"dasyurid marsupial\"\n",
            "Scraping images for class \"sexton\"\n",
            "Scraping images for class \"horse trader\"\n",
            "Scraping images for class \"pitching wedge\"\n",
            "Scraping images for class \"Elamite\"\n",
            "Scraping images for class \"horse wrangler\"\n",
            "Scraping images for class \"Brabancon griffon\"\n",
            "Scraping images for class \"farm boy\"\n",
            "Scraping images for class \"whooper\"\n",
            "Scraping images for class \"annual fern\"\n",
            "Scraping images for class \"jewels-of-opar\"\n",
            "Scraping images for class \"windshield\"\n",
            "Scraping images for class \"mosquitofish\"\n",
            "Scraping images for class \"two-eyed violet\"\n",
            "Scraping images for class \"blanket\"\n",
            "Scraping images for class \"etui\"\n",
            "Scraping images for class \"pinstripe\"\n",
            "Scraping images for class \"feather boa\"\n",
            "Scraping images for class \"girder\"\n",
            "Scraping images for class \"Confederate\"\n",
            "Scraping images for class \"silverspot\"\n",
            "Scraping images for class \"punch bowl\"\n",
            "Scraping images for class \"sowbread\"\n",
            "Scraping images for class \"chamberpot\"\n",
            "Scraping images for class \"high-protein diet\"\n",
            "Scraping images for class \"Orpington\"\n",
            "Scraping images for class \"western tanager\"\n",
            "Scraping images for class \"duffel\"\n",
            "Scraping images for class \"flat-coated retriever\"\n",
            "Scraping images for class \"emperor butterfly\"\n",
            "Scraping images for class \"Chilean firebush\"\n",
            "Scraping images for class \"automobile engine\"\n",
            "Scraping images for class \"screw\"\n",
            "Scraping images for class \"gill fungus\"\n",
            "Scraping images for class \"bladderpod\"\n",
            "Scraping images for class \"willowherb\"\n",
            "Scraping images for class \"wandering albatross\"\n",
            "Scraping images for class \"white sauce\"\n",
            "Scraping images for class \"potter wasp\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}